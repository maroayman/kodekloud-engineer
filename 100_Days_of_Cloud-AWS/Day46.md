## Task: Event-Driven Processing with Amazon S3 and Lambda
The DevOps team is working on automating file management between two S3 buckets. The task is to create a public S3 bucket for file uploads and a private S3 bucket for securely storing the files. A Lambda function will be triggered automatically whenever a file is uploaded to the public S3 bucket, which will copy the file to the private bucket. Additionally, logs of the operation will be stored in a DynamoDB table. The logs should include details such as the source bucket, destination bucket, and the object key of the file that was copied. This will help the team maintain better security and visibility for file transfers.

1. Create a public S3 bucket named `datacenter-public-30613`. Ensure that the bucket allows public access to its objects.
2. Create a private S3 bucket named `datacenter-private-21617`. Ensure that the bucket does not allow public access.
3. Create a Lambda function named `datacenter-copyfunction`. This function should be triggered by uploads to the public S3 bucket and should copy the uploaded file to the private bucket. Create the necessary policies and a role named `lambda_execution_role`. Attach these policies to the role, and then link this role to the Lambda function.
4. `lambda-function.py` is already present under the `/root/` directory on AWS client host, replace `REPLACE-WITH-YOUR-DYNAMODB-TABLE` and `REPLACE-WITH-YOUR-PRIVATE-BUCKET` values.
5. Create a DynamoDB table named `datacenter-S3CopyLogs` with a partition key `LogID` (string). This table will store logs generated by the Lambda function, including details such as source bucket name, destination bucket name, and object key.
6. For testing upload the file `sample.zip` located in the `/root` directory on the client host to the public S3 bucket. The Lambda function should trigger and copy the file to the private bucket.
7. Verify that the file has been successfully copied to the private bucket by checking the private bucket in the S3 console.
8. Verify that a log entry has been created in the DynamoDB table containing the file copy details.

---

## Solution

### Step 1: Set Variables
```bash
PUBLIC_S3="datacenter-public-30613"
PRIV_S3="datacenter-private-21617"
LAMBDA="datacenter-copyfunction"
LAMBDA_ROLE="lambda_execution_role"
DYNAMO_DB="datacenter-S3CopyLogs"
PARTITION_KEY="LogID"
```

### Step 2: Create Public S3 Bucket
Create bucket
```bash
aws s3api create-bucket \
  --bucket $PUBLIC_S3
```
Disable public access block
```bash
aws s3api put-public-access-block \
  --bucket $PUBLIC_S3 \
  --public-access-block-configuration \
  BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=false,RestrictPublicBuckets=false
```
Create bucket policy
```bash
cat <<EOF > public-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::$PUBLIC_S3/*"
    }
  ]
}
EOF
```
Apply it
```bash
aws s3api put-bucket-policy \
  --bucket $PUBLIC_S3 \
  --policy file://public-policy.json
```

### Step 3: Create Private S3 bucket
```bash
aws s3api create-bucket \
  --bucket $PRIV_S3
```

### Step 4: Create DynamoDB Table
```bash
aws dynamodb create-table \
  --table-name $DYNAMO_DB \
  --attribute-definitions AttributeName=$PARTITION_KEY,AttributeType=S \
  --key-schema AttributeName=$PARTITION_KEY,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
```
Wait until active
```bash
aws dynamodb wait table-exists --table-name $DYNAMO_DB
```

### Step 5: Create IAM Role for Lambda
Create `trust-policy.json` file
```bash
cat <<EOF > trust-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
```
Create role
```bash
aws iam create-role \
  --role-name $LAMBDA_ROLE \
  --assume-role-policy-document file://trust-policy.json
```

### Step 6: Create IAM Policy for S3 + DynamoDB
Create `lambda-policy.json` file
```bash
cat <<EOF > lambda-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::$PUBLIC_S3/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::$PRIV_S3/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem"
      ],
      "Resource": "arn:aws:dynamodb:*:*:table/$DYNAMO_DB"
    }
  ]
}
EOF
```
Create and attach policy
```bash
aws iam create-policy \
  --policy-name lambda_s3_dynamo_policy \
  --policy-document file://lambda-policy.json

# replace <ACCOUNT_ID> with actual ACCOUNT_ID
aws iam attach-role-policy \
  --role-name $LAMBDA_ROLE \
  --policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/lambda_s3_dynamo_policy
```

### Step 7: Create Lambda function
Update `/root/lambda-function.py` and replace placeholders with actual values
```
- <REPLACE-WITH-YOUR-DYNAMODB-TABLE>
- <REPLACE-WITH-YOUR-PRIVATE-BUCKET>
```
Zip lambda code
```bash
cd /root
zip lambda-function.zip lambda-function.py
```
Create lambda function
```bash
# replace <ACCOUNT_ID> with actual ACCOUNT_ID
aws lambda create-function \
  --function-name $LAMBDA \
  --runtime python3.9 \
  --role arn:aws:iam::<ACCOUNT_ID>:role/$LAMBDA_ROLE \
  --handler lambda-function.lambda_handler \
  --zip-file fileb://lambda-function.zip
```

### Step 8: Configure S3 Permission for Lambda
Add permission for S3 to invoke lambda
```bash
aws lambda add-permission \
  --function-name $LAMBDA \
  --statement-id s3invoke \
  --action lambda:InvokeFunction \
  --principal s3.amazonaws.com \
  --source-arn arn:aws:s3:::$PUBLIC_S3
```

### Step 9: Copy Lambda Function via AWS S3 CLI from AWS Client Host.
Upload `sample.zip` file from the AWS client host.
```bash
aws s3 cp /root/sample.zip s3://$PUBLIC_S3/
```

### Step 10: Configure bucket notification
Create `notification.json` file
```bash
# replace <ACCOUNT_ID> with actual ACCOUNT_ID
cat <<EOF > notification.json
{
  "LambdaFunctionConfigurations": [
    {
      "LambdaFunctionArn": "arn:aws:lambda:us-east-1:<ACCOUNT_ID>:function:$LAMBDA",
      "Events": ["s3:ObjectCreated:*"]
    }
  ]
}
EOF
```
Apply it
```bash
aws s3api put-bucket-notification-configuration \
  --bucket $PUBLIC_S3 \
  --notification-configuration file://notification.json
```

### Step 11: Verification
Check private bucket, you should see the `sample.zip` file

```bash
aws s3 ls s3://$PRIV_S3/
```
Verify DynamoDB entry
```bash
aws dynamodb scan --table-name $DYNAMO_DB
```
